{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1808cfb",
   "metadata": {},
   "source": [
    "Here, ANN model is trained on mel spectrogram calculated using librosa library. \n",
    "Mel feature contains both frequency and time aspects of the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd965c0b",
   "metadata": {},
   "source": [
    "- neccessary import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7d1092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa, os\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import pathlib\n",
    "from pydub import AudioSegment\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508192cc",
   "metadata": {},
   "source": [
    "### Place your dataset in the datas directory in the following format:\n",
    "\n",
    "- Copy the data from dataset directory\n",
    "- dataset directory contains data that is of 3/5 seconds interval. Choose the data of particular interval of interest.\n",
    "- E.g. if you want to train the model on 3 seconds interval data, copy directories from 'data_3_sec' and place it in datas folder in following format:\n",
    "\n",
    "datas\n",
    "\n",
    "    |-IVR\n",
    "        |-ivr.wav\n",
    "        |-ivr1.wav\n",
    "        |-ivr2.wav\n",
    "        |.........\n",
    "\n",
    "    |-Music\n",
    "        |-music.wav\n",
    "        |-music1.wav\n",
    "        |-music2.wav\n",
    "        |.........\n",
    "\n",
    "    |-Speech\n",
    "        |-speech.wav\n",
    "        |-speech2.wav\n",
    "        |-speech3.wav\n",
    "        |........."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7232f20",
   "metadata": {},
   "source": [
    "- join datas directory with absolute path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0b9f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/anush/Desktop/ac_new/dataset'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = pathlib.Path(__name__).parent.absolute()\n",
    "pdf_folder_path = os.path.join(base_dir,'dataset')\n",
    "pdf_folder_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba59b6",
   "metadata": {},
   "source": [
    "- create directory to store the spectrogram images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef63514",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR =base_dir\n",
    "if not os.path.exists(os.path.join(OUTPUT_DIR, 'audio-data-images')):\n",
    "    os.mkdir(os.path.join(OUTPUT_DIR, 'audio-data-images'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df0af91",
   "metadata": {},
   "source": [
    "- convert the mp3 files to wav, if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c4ff480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IVR\n",
      "Music\n",
      "Silence\n",
      "Speech\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(os.listdir(pdf_folder_path)):\n",
    "    print(i)\n",
    "    for j in glob.glob(os.path.join(OUTPUT_DIR, 'dataset',i,'*')):\n",
    "        if j.endswith('.wav'):\n",
    "            pass\n",
    "        else:\n",
    "            path =j\n",
    "            output_file = os.path.splitext(path)[0]\n",
    "            output_file= f'{output_file}.wav'\n",
    "            sound = AudioSegment.from_mp3(path)\n",
    "            sound.export(output_file, format=\"wav\")\n",
    "            os.remove(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b7b3a6",
   "metadata": {},
   "source": [
    "- convert the audio's to spectrogram images and save to audio-data-images directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801c17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sorted(os.listdir(pdf_folder_path)):\n",
    "    for audio_file in glob.glob(os.path.join(OUTPUT_DIR, 'dataset',i,'*'))[:3400]:\n",
    "        if not os.path.exists(os.path.join(OUTPUT_DIR, 'audio-data-images',i)):\n",
    "            os.mkdir(os.path.join(OUTPUT_DIR, 'audio-data-images',i))\n",
    "        file_stem = Path(audio_file).stem\n",
    "        image_file =os.path.join(OUTPUT_DIR, 'audio-data-images',i,file_stem)\n",
    "        file_exist = file_stem + '.png'\n",
    "        \n",
    "        if not os.path.exists(os.path.join(OUTPUT_DIR, 'audio-data-images',i,file_exist)):      \n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(1, 1, 1)\n",
    "            fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "            y, samp = librosa.load(audio_file, sr=8000)\n",
    "            ms = librosa.feature.melspectrogram(y=y, sr=samp)\n",
    "        \n",
    "            log_ms = librosa.power_to_db(ms, ref=np.max)\n",
    "            librosa.display.specshow(log_ms, sr=samp)\n",
    "\n",
    "            fig.savefig(f'{image_file}.png')\n",
    "            plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d2dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5164e4c",
   "metadata": {},
   "source": [
    "- load images from path as array and append it to list and its labels in other list\n",
    "- also show images after loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f64a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing import image\n",
    "# from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import keras.utils as image\n",
    "def load_images_from_path(path, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        images.append(image.img_to_array(image.load_img(os.path.join(path, file), target_size=(224, 224, 3))))\n",
    "        labels.append((label))\n",
    "        \n",
    "    return images, labels\n",
    "\n",
    "def show_images(images):\n",
    "    fig, axes = plt.subplots(1, 8, figsize=(20, 20), subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i] / 255)\n",
    "        \n",
    "x = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5ff4bd",
   "metadata": {},
   "source": [
    "- pass the ivr dataset path and get the image array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d90a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_path = os.path.join(base_dir,'audio-data-images','IVR')\n",
    "\n",
    "images, labels = load_images_from_path(music_path, 0)\n",
    "show_images(images)\n",
    "    \n",
    "x += images\n",
    "y += labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfed9869",
   "metadata": {},
   "source": [
    "- pass the music dataset path and get the image array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "Speech_path = os.path.join(base_dir,'audio-data-images','Music')\n",
    "images, labels = load_images_from_path(Speech_path, 1)\n",
    "show_images(images)\n",
    "    \n",
    "x += images\n",
    "y += labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf84b9fa",
   "metadata": {},
   "source": [
    "- pass the speech dataset path and get the image array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ba9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Silence_path = os.path.join(base_dir,'audio-data-images','Silence')\n",
    "images, labels = load_images_from_path(Silence_path, 2)\n",
    "show_images(images)\n",
    "    \n",
    "x += images\n",
    "y += labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef8c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "Speech_path = os.path.join(base_dir,'audio-data-images','Speech')\n",
    "images, labels = load_images_from_path(Speech_path, 3)\n",
    "show_images(images)\n",
    "    \n",
    "x += images\n",
    "y += labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605ac5a",
   "metadata": {},
   "source": [
    "- train test split at 70:30 ratio and label encoding to encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd557fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "x_train_norm = np.array(x_train) / 255\n",
    "x_test_norm = np.array(x_test) / 255\n",
    "\n",
    "y_train_encoded = to_categorical(y_train)\n",
    "y_test_encoded = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e0e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c319631",
   "metadata": {},
   "source": [
    "- to preprocess image keras is used\n",
    "- to extract features, mobilenetv2 is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0778eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = VGG19( include_top=False, weights=\"imagenet\",input_shape=(224, 224, 3)) #half size model to MV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aab21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "x_train_norm = preprocess_input(np.array(x_train))\n",
    "x_test_norm = preprocess_input(np.array(x_test))\n",
    "\n",
    "train_features = base_model.predict(x_train_norm)\n",
    "test_features = base_model.predict(x_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f688485",
   "metadata": {},
   "source": [
    "### model creation & training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893f580",
   "metadata": {},
   "source": [
    "- sequential model is used with adam optimizer and categorical cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e3a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=train_features.shape[1:]))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8088ce6a",
   "metadata": {},
   "source": [
    "- model is trained with batch size of 10 and 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3affe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit(train_features, y_train_encoded, validation_data=(test_features, y_test_encoded), batch_size=10, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdeb030",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ann_mobilenetv2_1sec_normalized.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f57b44",
   "metadata": {},
   "source": [
    "### model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4f61d3",
   "metadata": {},
   "source": [
    "- plot training and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b875ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = hist.history['accuracy']\n",
    "val_acc = hist.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, '-', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, ':', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2180a80d",
   "metadata": {},
   "source": [
    "- plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec66e054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "y_predicted = model.predict(test_features)\n",
    "mat = confusion_matrix(y_test_encoded.argmax(axis=1), y_predicted.argmax(axis=1))\n",
    "class_labels = ['IVR', 'Music', 'Silence', 'Speech']\n",
    "\n",
    "sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Blues',\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels)\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('Actual label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdf44bb",
   "metadata": {},
   "source": [
    "- print accuracy, recall, F1 score, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832eef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report =classification_report(y_test_encoded.argmax(axis=1), y_predicted.argmax(axis=1))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9483313d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93c8d51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
