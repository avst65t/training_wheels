{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python mtcnn tensorflow torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from arcface.model import ArcFaceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mtcnn_model():\n",
    "    return MTCNN()\n",
    "\n",
    "mtcnn_model = load_mtcnn_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arcface_model():\n",
    "    model = ArcFaceModel(size=112, backbone_type='ResNet50', model_path='path/to/arcface_model_weights.pth')\n",
    "    return model\n",
    "\n",
    "arcface_model = load_arcface_model()\n",
    "arcface_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((112, 112)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def face_recognition(frame):\n",
    "    # Detect faces using MTCNN\n",
    "    faces = mtcnn_model.detect_faces(frame)\n",
    "    \n",
    "    for face in faces:\n",
    "        x, y, w, h = face['box']\n",
    "        roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "        # Preprocess the face for ArcFace model\n",
    "        input_face = preprocess_image(roi)\n",
    "        input_face = input_face.unsqueeze(0)\n",
    "\n",
    "        # Extract features using ArcFace\n",
    "        with torch.no_grad():\n",
    "            embedding = arcface_model(input_face)\n",
    "\n",
    "        # Perform face recognition using the embeddings (you'll need a database for this)\n",
    "        recognized_person = recognize_person(embedding)\n",
    "\n",
    "        # Draw bounding box and label on the frame\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, recognized_person, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "def recognize_person(embedding):\n",
    "    # Implement your logic for face recognition based on the embeddings\n",
    "    # You might compare the embeddings with a database of known faces\n",
    "    # and return the name or ID of the recognized person.\n",
    "    return \"John Doe\"\n",
    "\n",
    "# Open a video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform face recognition on the frame\n",
    "    frame = face_recognition(frame)\n",
    "\n",
    "    # Display the result\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_train_svm_model():\n",
    "    svm_model_path = 'path/to/your/svm_model.pkl'\n",
    "\n",
    "    if os.path.exists(svm_model_path):\n",
    "        # Load pre-trained SVM model\n",
    "        svm_model = joblib.load(svm_model_path)\n",
    "        label_encoder = joblib.load('path/to/your/label_encoder.pkl')\n",
    "    else:\n",
    "        # Train SVM model using your dataset\n",
    "        # Assume you have a list of face embeddings (X) and corresponding labels (y)\n",
    "        # X and y should be obtained from your face recognition database\n",
    "        # You need to collect embeddings and labels for known individuals\n",
    "        X, y = collect_embeddings_and_labels()\n",
    "\n",
    "        # Encode labels\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "        # Train SVM model\n",
    "        svm_model = SVC(C=1.0, kernel='linear', probability=True)\n",
    "        svm_model.fit(X, y_encoded)\n",
    "\n",
    "        # Save the trained model and label encoder for future use\n",
    "        joblib.dump(svm_model, svm_model_path)\n",
    "        joblib.dump(label_encoder, 'path/to/your/label_encoder.pkl')\n",
    "\n",
    "    return svm_model, label_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_embeddings_and_labels():\n",
    "    # Replace this with your logic to collect face embeddings and labels\n",
    "    # You need to have a list of face embeddings (X) and corresponding labels (y)\n",
    "    # X and y should be obtained from your face recognition database\n",
    "    # Each row of X should be an embedding, and each element of y should be the label for that embedding\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Example: Iterate through your dataset and append embeddings and labels\n",
    "    # for img_path, label in your_dataset:\n",
    "    #     embedding = compute_embedding(img_path)  # You need to implement this\n",
    "    #     X.append(embedding)\n",
    "    #     y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_person(embedding, svm_model, label_encoder):\n",
    "    # Use the SVM model to predict the label for the given embedding\n",
    "    probas = svm_model.predict_proba([embedding])\n",
    "    predicted_class = np.argmax(probas)\n",
    "    confidence = probas[0, predicted_class]\n",
    "\n",
    "    # Convert the predicted class back to the original label using the label encoder\n",
    "    recognized_person = label_encoder.inverse_transform([predicted_class])[0]\n",
    "\n",
    "    # You can use confidence to set a threshold for recognition confidence\n",
    "    # For example, only recognize if confidence is above a certain threshold\n",
    "    threshold = 0.8\n",
    "    if confidence < threshold:\n",
    "        recognized_person = \"Unknown\"\n",
    "\n",
    "    return recognized_person\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...\n",
    "\n",
    "# Load or train SVM model\n",
    "svm_model, label_encoder = load_or_train_svm_model()\n",
    "\n",
    "# ...\n",
    "\n",
    "def face_recognition(frame):\n",
    "    # ...\n",
    "\n",
    "    for face in faces:\n",
    "        # ...\n",
    "\n",
    "        # Perform face recognition using the SVM model\n",
    "        recognized_person = recognize_person(embedding, svm_model, label_encoder)\n",
    "\n",
    "        # ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
