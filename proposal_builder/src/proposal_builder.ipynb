{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5012d692-7c9e-49d9-a8ce-1293e367f199",
   "metadata": {},
   "source": [
    "### Transcript Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8720779-12ef-4c2a-948c-abb60f330dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path: str) -> str:\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"file not here: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"error: {file_path}\")\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c688aa3d-b10e-4726-8a23-446db37cc912",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript=read_text_file(\"transcripts/rag.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c8d406-0710-442e-8e4a-caaeffb0b69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transcript: RAG Agentic Bot Requirements Call\\n\\nParticipants:\\nRohit (Client, Head of Innovation at a '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940d7f21-d0db-4bbe-8dc7-d7cf248998db",
   "metadata": {},
   "source": [
    "### Text processing Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda72c50-3ce4-48ca-b000-be76af2d43a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text_for_llm(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = \"\".join(ch for ch in text if ch.isprintable() or ch in \"\\n\\t\")\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    paragraphs = re.split(r\"\\n{2,}\", text)\n",
    "    paragraphs = [\" \".join(p.splitlines()).strip() for p in paragraphs if p.strip()]\n",
    "    cleaned = \"\\n\\n\".join(paragraphs)\n",
    "    return cleaned.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a1a934-ed24-48fe-a772-a1491c4ba18b",
   "metadata": {},
   "source": [
    "### LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869b92ab-ed51-435f-8f60-0a397e1ca58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key= \"\"\n",
    "\n",
    "def call_llm(model: str, system_prompt: str, user_prompt: str, temperature: float) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                  {\"role\": \"user\", \"content\": user_prompt},],\n",
    "        temperature=temperature,\n",
    "        stream=True)\n",
    "    result=\"\"\n",
    "    for chunk in response:\n",
    "        if 'choices' in chunk and chunk['choices'][0].get('delta'):\n",
    "            content = chunk['choices'][0]['delta'].get(content)\n",
    "            if content:\n",
    "                print(content, end=\"\", flush=True)\n",
    "                result+=content\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ebb25-4b9d-41b6-be1d-4f4533729803",
   "metadata": {},
   "source": [
    "### Solution Architect Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c402a5ef-2650-41d9-b66a-56dfee7569c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_architect_system_prompt=\"\"\"\n",
    "You are acting as a senior solution architect. \n",
    "I will give you a transcript of a client call discussing project requirements. \n",
    "Your job is to produce a clear, structured, and detailed requirement analysis that can directly feed into a proposal. \n",
    "Make it simple but comprehensive, so that each part can expand into proposal sections later.\n",
    "\n",
    "Cover the following:\n",
    "\n",
    "1. Business Context & Problem\n",
    "- What is the client’s domain and pain points?\n",
    "- What exactly do they want to achieve?\n",
    "\n",
    "2. Functional Requirements\n",
    "- List each requirement explicitly as features or modules.\n",
    "- Separate \"must-have\" vs \"good-to-have\".\n",
    "\n",
    "3. Non-Functional Requirements\n",
    "- Security, scalability, reliability, compliance, cost considerations.\n",
    "\n",
    "4. End-to-End Flow (Step by Step)\n",
    "Explain the process like a journey:\n",
    "User → Interface → Backend services → Databases → AI/ML modules → External integrations → Output.  \n",
    "Show the sequence clearly, in numbered steps.\n",
    "\n",
    "5. Recommended Tech Stack & Services\n",
    "- Frontend frameworks\n",
    "- Backend frameworks\n",
    "- Databases (SQL/NoSQL/vector stores)\n",
    "- AI/ML services (if relevant)\n",
    "- Cloud provider + services (compute, storage, monitoring, CI/CD, networking)\n",
    "- Third-party integrations (payments, notifications, APIs)\n",
    "\n",
    "6. High-Level Architecture\n",
    "- Suggest how components connect: frontend, API gateway, microservices, DB, AI engine, integrations.\n",
    "- Mention best practices (e.g., event-driven, modular, multi-tenant).\n",
    "\n",
    "7. Deployment & Infrastructure Strategy\n",
    "- Cloud choice and region\n",
    "- Containers or serverless\n",
    "- CI/CD pipeline\n",
    "- Monitoring, logging, and observability stack\n",
    "\n",
    "8. Phased Implementation Roadmap\n",
    "- Break project into realistic phases (MVP, Expansion, Advanced).\n",
    "- Tie each phase to deliverables and business value.\n",
    "\n",
    "9. Best Strategy\n",
    "- State the *recommended approach* (e.g., RAG-based retrieval for AI, modular SaaS design, serverless for cost efficiency, etc.)\n",
    "- Why this approach is best in terms of cost, scalability, and maintainability.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4cb05-657a-4ba4-9216-45c96c308fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_architecture = call_llm(\"o-reasoning-1\", solution_architect_system_prompt, solution_architect_user_prompt, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dacb33b-b314-4e95-856f-355f9355b552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d586b-72b9-46e9-b54f-67d2ee6290da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3893220e-d803-4952-9f99-f730456460f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b6134-70a7-4d06-8ddc-6b26e38197de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795c8a3-9622-4744-a99f-32690c8d7a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf9db8-a4e0-4e13-a2b3-99cfcf9fedb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770864e-42eb-4ca2-a6d0-66cb57f95952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b6cf3-080e-487b-86a8-eb7e023a3255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b279ccce-2e17-4c64-bf55-675ce467b340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
